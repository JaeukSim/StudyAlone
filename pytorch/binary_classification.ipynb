{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "torch.manual_seed(1)\r\n",
    "\r\n",
    "# train data tensor\r\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3],[5,3],[6,2]]\r\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\r\n",
    "\r\n",
    "# train data - > tensor\r\n",
    "x_train = torch.FloatTensor(x_data)\r\n",
    "y_train = torch.FloatTensor(y_data)\r\n",
    "\r\n",
    "model = nn.Sequential(\r\n",
    "    nn.Linear(2,1), # input dim 2 output dim 1\r\n",
    "    nn.Sigmoid() # output 시그모이드 함수를 거친다.\r\n",
    "    )\r\n",
    "model(x_train)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4020],\n",
       "        [0.4147],\n",
       "        [0.6556],\n",
       "        [0.5948],\n",
       "        [0.6788],\n",
       "        [0.8061]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 6X1 크기의 예측값 텐서가 출력\r\n",
    "# 예측은 의미가 없다.\r\n",
    "# 경사 하강법 \r\n",
    "\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\r\n",
    "epoch_number =100\r\n",
    "\r\n",
    "# train loop\r\n",
    "for epoch in range(epoch_number+1):\r\n",
    "    # H(x) 계산\r\n",
    "    hypothesis = model(x_train)\r\n",
    "\r\n",
    "    # loss\r\n",
    "    loss = F.binary_cross_entropy(hypothesis, y_train)\r\n",
    "\r\n",
    "    # loss H(x)\r\n",
    "    optimizer.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    #print\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        # 예측값이 0.5 넘으면 True로 간주함\r\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\r\n",
    "\r\n",
    "        # 실제값과 일치하는 경우만 True로 간주함\r\n",
    "        correct_prediction = prediction.float() == y_train\r\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\r\n",
    "        print(\"Epoch {:4d}/{} cost : {:.6f} Acc. {:.2f}%\".format(epoch, epoch_number, loss.item(), accuracy*100))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/100 cost : 0.539713 Acc. 83.33%\n",
      "Epoch   10/100 cost : 0.510919 Acc. 83.33%\n",
      "Epoch   20/100 cost : 0.490904 Acc. 83.33%\n",
      "Epoch   30/100 cost : 0.475095 Acc. 83.33%\n",
      "Epoch   40/100 cost : 0.461999 Acc. 83.33%\n",
      "Epoch   50/100 cost : 0.450719 Acc. 83.33%\n",
      "Epoch   60/100 cost : 0.440698 Acc. 83.33%\n",
      "Epoch   70/100 cost : 0.431580 Acc. 83.33%\n",
      "Epoch   80/100 cost : 0.423135 Acc. 83.33%\n",
      "Epoch   90/100 cost : 0.415206 Acc. 83.33%\n",
      "Epoch  100/100 cost : 0.407688 Acc. 83.33%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model(x_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1992],\n",
       "        [0.2456],\n",
       "        [0.6925],\n",
       "        [0.6284],\n",
       "        [0.7940],\n",
       "        [0.9387]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 실제값은 [[0],[0],[0],[1],[1],[1]]\r\n",
    "# 이는 FFF TTT 에 해당되므로 전부\r\n",
    "# 실제값과 일치하도록 예측한 것을 확인할 수 있습니다."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}