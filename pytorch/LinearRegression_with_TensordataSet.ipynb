{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# DataLoader\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "# TensorDataset and DataLoader\r\n",
    "from torch.utils.data import TensorDataset # 텐서 데이터 셋\r\n",
    "from torch.utils.data import DataLoader # 데이터 로더\r\n",
    "\r\n",
    "# TensorDataset은 기본적으로 텐서를 입력받습니다. 텐서 형태로 데이터를 정의\r\n",
    "\r\n",
    "x_train = torch.FloatTensor([\r\n",
    "    [73, 80, 75],\r\n",
    "    [93, 88, 93],\r\n",
    "    [89, 91, 90],\r\n",
    "    [96, 98, 100],\r\n",
    "    [73, 66, 70]\r\n",
    "    ])\r\n",
    "\r\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196], [142]])\r\n",
    "\r\n",
    "# TensorDataset의 입력으로 사용하고 dataset을 지정합니다.\r\n",
    "dataset = TensorDataset(x_train, y_train)\r\n",
    "\r\n",
    "# dataLoader\r\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\r\n",
    "\r\n",
    "# 모델과 옵티마이저 설계\r\n",
    "model = nn.Linear(3,1)\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\r\n",
    "\r\n",
    "epoch_nb=500\r\n",
    "for epoch in range(epoch_nb+1):\r\n",
    "    for batch_idx, samples in enumerate(dataloader):\r\n",
    "        x_train, y_train = samples\r\n",
    "\r\n",
    "        # H(X) 계산\r\n",
    "        prediction = model(x_train)\r\n",
    "        # loss\r\n",
    "        loss = F.mse_loss(prediction, y_train)\r\n",
    "\r\n",
    "        # loss H(x) 3가지 짝궁\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        if epoch % 10 ==0 :\r\n",
    "            print(\"Epoch {:4d}/{} Batch {}/{} Loss : {:.6f}\".format(epoch, epoch_nb, batch_idx+1, len(dataloader), loss.item()))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/500 Batch 1/3 Loss : 26272.300781\n",
      "Epoch    0/500 Batch 2/3 Loss : 3561.269287\n",
      "Epoch    0/500 Batch 3/3 Loss : 2881.196045\n",
      "Epoch   10/500 Batch 1/3 Loss : 7.390985\n",
      "Epoch   10/500 Batch 2/3 Loss : 4.101743\n",
      "Epoch   10/500 Batch 3/3 Loss : 3.719278\n",
      "Epoch   20/500 Batch 1/3 Loss : 1.914150\n",
      "Epoch   20/500 Batch 2/3 Loss : 7.518993\n",
      "Epoch   20/500 Batch 3/3 Loss : 9.096203\n",
      "Epoch   30/500 Batch 1/3 Loss : 4.586367\n",
      "Epoch   30/500 Batch 2/3 Loss : 4.862072\n",
      "Epoch   30/500 Batch 3/3 Loss : 8.580565\n",
      "Epoch   40/500 Batch 1/3 Loss : 7.258605\n",
      "Epoch   40/500 Batch 2/3 Loss : 5.248997\n",
      "Epoch   40/500 Batch 3/3 Loss : 0.364139\n",
      "Epoch   50/500 Batch 1/3 Loss : 0.307228\n",
      "Epoch   50/500 Batch 2/3 Loss : 7.454300\n",
      "Epoch   50/500 Batch 3/3 Loss : 10.696221\n",
      "Epoch   60/500 Batch 1/3 Loss : 6.662453\n",
      "Epoch   60/500 Batch 2/3 Loss : 3.587806\n",
      "Epoch   60/500 Batch 3/3 Loss : 3.762011\n",
      "Epoch   70/500 Batch 1/3 Loss : 5.919252\n",
      "Epoch   70/500 Batch 2/3 Loss : 4.848046\n",
      "Epoch   70/500 Batch 3/3 Loss : 6.734507\n",
      "Epoch   80/500 Batch 1/3 Loss : 2.742970\n",
      "Epoch   80/500 Batch 2/3 Loss : 5.466687\n",
      "Epoch   80/500 Batch 3/3 Loss : 8.415984\n",
      "Epoch   90/500 Batch 1/3 Loss : 0.899985\n",
      "Epoch   90/500 Batch 2/3 Loss : 9.597122\n",
      "Epoch   90/500 Batch 3/3 Loss : 9.765339\n",
      "Epoch  100/500 Batch 1/3 Loss : 6.959698\n",
      "Epoch  100/500 Batch 2/3 Loss : 4.586006\n",
      "Epoch  100/500 Batch 3/3 Loss : 1.410736\n",
      "Epoch  110/500 Batch 1/3 Loss : 6.963066\n",
      "Epoch  110/500 Batch 2/3 Loss : 6.619991\n",
      "Epoch  110/500 Batch 3/3 Loss : 6.524713\n",
      "Epoch  120/500 Batch 1/3 Loss : 0.658122\n",
      "Epoch  120/500 Batch 2/3 Loss : 6.337847\n",
      "Epoch  120/500 Batch 3/3 Loss : 8.683047\n",
      "Epoch  130/500 Batch 1/3 Loss : 8.768326\n",
      "Epoch  130/500 Batch 2/3 Loss : 3.593596\n",
      "Epoch  130/500 Batch 3/3 Loss : 4.978871\n",
      "Epoch  140/500 Batch 1/3 Loss : 2.007591\n",
      "Epoch  140/500 Batch 2/3 Loss : 5.108857\n",
      "Epoch  140/500 Batch 3/3 Loss : 8.124970\n",
      "Epoch  150/500 Batch 1/3 Loss : 6.508489\n",
      "Epoch  150/500 Batch 2/3 Loss : 8.041658\n",
      "Epoch  150/500 Batch 3/3 Loss : 0.038213\n",
      "Epoch  160/500 Batch 1/3 Loss : 2.595279\n",
      "Epoch  160/500 Batch 2/3 Loss : 5.675857\n",
      "Epoch  160/500 Batch 3/3 Loss : 6.633596\n",
      "Epoch  170/500 Batch 1/3 Loss : 4.683478\n",
      "Epoch  170/500 Batch 2/3 Loss : 5.554266\n",
      "Epoch  170/500 Batch 3/3 Loss : 2.252702\n",
      "Epoch  180/500 Batch 1/3 Loss : 5.615547\n",
      "Epoch  180/500 Batch 2/3 Loss : 6.150796\n",
      "Epoch  180/500 Batch 3/3 Loss : 5.836763\n",
      "Epoch  190/500 Batch 1/3 Loss : 0.808182\n",
      "Epoch  190/500 Batch 2/3 Loss : 8.229963\n",
      "Epoch  190/500 Batch 3/3 Loss : 7.993027\n",
      "Epoch  200/500 Batch 1/3 Loss : 5.309777\n",
      "Epoch  200/500 Batch 2/3 Loss : 4.070264\n",
      "Epoch  200/500 Batch 3/3 Loss : 0.317695\n",
      "Epoch  210/500 Batch 1/3 Loss : 3.896549\n",
      "Epoch  210/500 Batch 2/3 Loss : 5.163899\n",
      "Epoch  210/500 Batch 3/3 Loss : 0.292384\n",
      "Epoch  220/500 Batch 1/3 Loss : 2.085501\n",
      "Epoch  220/500 Batch 2/3 Loss : 10.610073\n",
      "Epoch  220/500 Batch 3/3 Loss : 0.715575\n",
      "Epoch  230/500 Batch 1/3 Loss : 2.921758\n",
      "Epoch  230/500 Batch 2/3 Loss : 5.042173\n",
      "Epoch  230/500 Batch 3/3 Loss : 5.098682\n",
      "Epoch  240/500 Batch 1/3 Loss : 5.445832\n",
      "Epoch  240/500 Batch 2/3 Loss : 3.674090\n",
      "Epoch  240/500 Batch 3/3 Loss : 0.155739\n",
      "Epoch  250/500 Batch 1/3 Loss : 3.534274\n",
      "Epoch  250/500 Batch 2/3 Loss : 2.460243\n",
      "Epoch  250/500 Batch 3/3 Loss : 7.766533\n",
      "Epoch  260/500 Batch 1/3 Loss : 5.076492\n",
      "Epoch  260/500 Batch 2/3 Loss : 3.971816\n",
      "Epoch  260/500 Batch 3/3 Loss : 2.479606\n",
      "Epoch  270/500 Batch 1/3 Loss : 5.007692\n",
      "Epoch  270/500 Batch 2/3 Loss : 3.265276\n",
      "Epoch  270/500 Batch 3/3 Loss : 0.884781\n",
      "Epoch  280/500 Batch 1/3 Loss : 2.069710\n",
      "Epoch  280/500 Batch 2/3 Loss : 9.515924\n",
      "Epoch  280/500 Batch 3/3 Loss : 0.288044\n",
      "Epoch  290/500 Batch 1/3 Loss : 2.023320\n",
      "Epoch  290/500 Batch 2/3 Loss : 3.263895\n",
      "Epoch  290/500 Batch 3/3 Loss : 6.280478\n",
      "Epoch  300/500 Batch 1/3 Loss : 2.939104\n",
      "Epoch  300/500 Batch 2/3 Loss : 3.245951\n",
      "Epoch  300/500 Batch 3/3 Loss : 4.928270\n",
      "Epoch  310/500 Batch 1/3 Loss : 1.741100\n",
      "Epoch  310/500 Batch 2/3 Loss : 3.493304\n",
      "Epoch  310/500 Batch 3/3 Loss : 6.064177\n",
      "Epoch  320/500 Batch 1/3 Loss : 4.530730\n",
      "Epoch  320/500 Batch 2/3 Loss : 2.400759\n",
      "Epoch  320/500 Batch 3/3 Loss : 2.582677\n",
      "Epoch  330/500 Batch 1/3 Loss : 3.243514\n",
      "Epoch  330/500 Batch 2/3 Loss : 4.239094\n",
      "Epoch  330/500 Batch 3/3 Loss : 0.224993\n",
      "Epoch  340/500 Batch 1/3 Loss : 3.380017\n",
      "Epoch  340/500 Batch 2/3 Loss : 4.034691\n",
      "Epoch  340/500 Batch 3/3 Loss : 2.185061\n",
      "Epoch  350/500 Batch 1/3 Loss : 0.438865\n",
      "Epoch  350/500 Batch 2/3 Loss : 6.966680\n",
      "Epoch  350/500 Batch 3/3 Loss : 5.544424\n",
      "Epoch  360/500 Batch 1/3 Loss : 4.327582\n",
      "Epoch  360/500 Batch 2/3 Loss : 2.425775\n",
      "Epoch  360/500 Batch 3/3 Loss : 4.088620\n",
      "Epoch  370/500 Batch 1/3 Loss : 3.877707\n",
      "Epoch  370/500 Batch 2/3 Loss : 2.978498\n",
      "Epoch  370/500 Batch 3/3 Loss : 0.570701\n",
      "Epoch  380/500 Batch 1/3 Loss : 3.249529\n",
      "Epoch  380/500 Batch 2/3 Loss : 1.284105\n",
      "Epoch  380/500 Batch 3/3 Loss : 7.327688\n",
      "Epoch  390/500 Batch 1/3 Loss : 3.020265\n",
      "Epoch  390/500 Batch 2/3 Loss : 3.020784\n",
      "Epoch  390/500 Batch 3/3 Loss : 3.810586\n",
      "Epoch  400/500 Batch 1/3 Loss : 3.120924\n",
      "Epoch  400/500 Batch 2/3 Loss : 1.573285\n",
      "Epoch  400/500 Batch 3/3 Loss : 5.534009\n",
      "Epoch  410/500 Batch 1/3 Loss : 4.026790\n",
      "Epoch  410/500 Batch 2/3 Loss : 5.445477\n",
      "Epoch  410/500 Batch 3/3 Loss : 0.027749\n",
      "Epoch  420/500 Batch 1/3 Loss : 3.459675\n",
      "Epoch  420/500 Batch 2/3 Loss : 3.017492\n",
      "Epoch  420/500 Batch 3/3 Loss : 0.217942\n",
      "Epoch  430/500 Batch 1/3 Loss : 2.187752\n",
      "Epoch  430/500 Batch 2/3 Loss : 3.456369\n",
      "Epoch  430/500 Batch 3/3 Loss : 2.258843\n",
      "Epoch  440/500 Batch 1/3 Loss : 6.061418\n",
      "Epoch  440/500 Batch 2/3 Loss : 1.982035\n",
      "Epoch  440/500 Batch 3/3 Loss : 2.484510\n",
      "Epoch  450/500 Batch 1/3 Loss : 1.529362\n",
      "Epoch  450/500 Batch 2/3 Loss : 4.579145\n",
      "Epoch  450/500 Batch 3/3 Loss : 4.967234\n",
      "Epoch  460/500 Batch 1/3 Loss : 0.768670\n",
      "Epoch  460/500 Batch 2/3 Loss : 3.107774\n",
      "Epoch  460/500 Batch 3/3 Loss : 5.653679\n",
      "Epoch  470/500 Batch 1/3 Loss : 4.539135\n",
      "Epoch  470/500 Batch 2/3 Loss : 2.387853\n",
      "Epoch  470/500 Batch 3/3 Loss : 2.645636\n",
      "Epoch  480/500 Batch 1/3 Loss : 1.103513\n",
      "Epoch  480/500 Batch 2/3 Loss : 2.777176\n",
      "Epoch  480/500 Batch 3/3 Loss : 4.961250\n",
      "Epoch  490/500 Batch 1/3 Loss : 4.755074\n",
      "Epoch  490/500 Batch 2/3 Loss : 2.781588\n",
      "Epoch  490/500 Batch 3/3 Loss : 1.338714\n",
      "Epoch  500/500 Batch 1/3 Loss : 2.900149\n",
      "Epoch  500/500 Batch 2/3 Loss : 3.449368\n",
      "Epoch  500/500 Batch 3/3 Loss : 5.248054\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 모델의 입력으로 임의의 값을 주고 예측값을 확인\r\n",
    "# 임의의 값\r\n",
    "test_val = torch.FloatTensor([[96, 98, 100]])\r\n",
    "pred_y = model(test_val)\r\n",
    "print(\"훈련 후 입력이 73, 80, 75 일때 예측 값 : \", pred_y)\r\n",
    "print(1e-5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 후 입력이 73, 80, 75 일때 예측 값 :  tensor([[196.1832]], grad_fn=<AddmmBackward>)\n",
      "1e-05\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}