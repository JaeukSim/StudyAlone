{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 다중선형 회귀\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "torch.manual_seed(777)\r\n",
    "\r\n",
    "# data\r\n",
    "x1_train = torch.FloatTensor(([73],[93], [89], [96], [73]))\r\n",
    "x2_train = torch.FloatTensor(([80],[88], [91], [98], [66]))\r\n",
    "x3_train = torch.FloatTensor(([75],[93], [90], [100], [70]))\r\n",
    "\r\n",
    "# 정답지\r\n",
    "y_train = torch.FloatTensor(([152], [185], [180], [196], [142])) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 가중치 w와 편향 b를 선언\r\n",
    "w1 = torch.zeros(1, requires_grad=True)\r\n",
    "w2 = torch.zeros(1, requires_grad=True)\r\n",
    "w3 = torch.zeros(1, requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\r\n",
    "epoch_num = 1000\r\n",
    "for epoch in range(0, epoch_num+1):\r\n",
    "    # H(x)를 계산\r\n",
    "    # 가설을 선언 부분\r\n",
    "    # hypothesis = x1_train*w1 + x2_train*w2 +x3_train*w3 +b\r\n",
    "    hypothesis = x1_train*w1 + x2_train*w2 +x3_train*w3 + b\r\n",
    "\r\n",
    "    #loss \r\n",
    "    loss= torch.mean((hypothesis- y_train)**2)\r\n",
    "\r\n",
    "    # loss H(x) 계산\r\n",
    "    optimizer.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    # 100마다 print\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print(\"Epoch {:4d}/{} w1 {:.3f} w2 {:.3f} w3 {:.3f} b :{:.3f} loss : {:.6f}\".format(epoch, epoch_num, w1.item(), w2.item(), w3.item(), b.item(), loss.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 w1 0.294 w2 0.294 w3 0.297 b :0.003 loss : 29661.800781\n",
      "Epoch   10/1000 w1 0.667 w2 0.665 w3 0.675 b :0.008 loss : 1.897688\n",
      "Epoch   20/1000 w1 0.669 w2 0.666 w3 0.676 b :0.008 loss : 1.619039\n",
      "Epoch   30/1000 w1 0.670 w2 0.665 w3 0.676 b :0.008 loss : 1.611993\n",
      "Epoch   40/1000 w1 0.670 w2 0.665 w3 0.676 b :0.008 loss : 1.604973\n",
      "Epoch   50/1000 w1 0.671 w2 0.664 w3 0.676 b :0.008 loss : 1.597991\n",
      "Epoch   60/1000 w1 0.671 w2 0.663 w3 0.676 b :0.008 loss : 1.591035\n",
      "Epoch   70/1000 w1 0.672 w2 0.663 w3 0.676 b :0.008 loss : 1.584131\n",
      "Epoch   80/1000 w1 0.672 w2 0.662 w3 0.676 b :0.008 loss : 1.577264\n",
      "Epoch   90/1000 w1 0.673 w2 0.662 w3 0.676 b :0.008 loss : 1.570433\n",
      "Epoch  100/1000 w1 0.674 w2 0.661 w3 0.676 b :0.008 loss : 1.563634\n",
      "Epoch  110/1000 w1 0.674 w2 0.660 w3 0.676 b :0.008 loss : 1.556869\n",
      "Epoch  120/1000 w1 0.675 w2 0.660 w3 0.676 b :0.008 loss : 1.550140\n",
      "Epoch  130/1000 w1 0.675 w2 0.659 w3 0.676 b :0.008 loss : 1.543452\n",
      "Epoch  140/1000 w1 0.676 w2 0.659 w3 0.676 b :0.008 loss : 1.536793\n",
      "Epoch  150/1000 w1 0.676 w2 0.658 w3 0.677 b :0.008 loss : 1.530171\n",
      "Epoch  160/1000 w1 0.677 w2 0.657 w3 0.677 b :0.008 loss : 1.523593\n",
      "Epoch  170/1000 w1 0.677 w2 0.657 w3 0.677 b :0.008 loss : 1.517047\n",
      "Epoch  180/1000 w1 0.678 w2 0.656 w3 0.677 b :0.008 loss : 1.510532\n",
      "Epoch  190/1000 w1 0.678 w2 0.656 w3 0.677 b :0.008 loss : 1.504045\n",
      "Epoch  200/1000 w1 0.679 w2 0.655 w3 0.677 b :0.008 loss : 1.497607\n",
      "Epoch  210/1000 w1 0.679 w2 0.654 w3 0.677 b :0.008 loss : 1.491201\n",
      "Epoch  220/1000 w1 0.680 w2 0.654 w3 0.677 b :0.008 loss : 1.484829\n",
      "Epoch  230/1000 w1 0.681 w2 0.653 w3 0.677 b :0.008 loss : 1.478481\n",
      "Epoch  240/1000 w1 0.681 w2 0.653 w3 0.677 b :0.008 loss : 1.472174\n",
      "Epoch  250/1000 w1 0.682 w2 0.652 w3 0.677 b :0.008 loss : 1.465901\n",
      "Epoch  260/1000 w1 0.682 w2 0.651 w3 0.677 b :0.008 loss : 1.459648\n",
      "Epoch  270/1000 w1 0.683 w2 0.651 w3 0.677 b :0.008 loss : 1.453449\n",
      "Epoch  280/1000 w1 0.683 w2 0.650 w3 0.677 b :0.008 loss : 1.447283\n",
      "Epoch  290/1000 w1 0.684 w2 0.650 w3 0.677 b :0.008 loss : 1.441141\n",
      "Epoch  300/1000 w1 0.684 w2 0.649 w3 0.677 b :0.008 loss : 1.435026\n",
      "Epoch  310/1000 w1 0.685 w2 0.649 w3 0.677 b :0.008 loss : 1.428954\n",
      "Epoch  320/1000 w1 0.685 w2 0.648 w3 0.677 b :0.008 loss : 1.422918\n",
      "Epoch  330/1000 w1 0.686 w2 0.647 w3 0.677 b :0.008 loss : 1.416895\n",
      "Epoch  340/1000 w1 0.686 w2 0.647 w3 0.678 b :0.008 loss : 1.410926\n",
      "Epoch  350/1000 w1 0.687 w2 0.646 w3 0.678 b :0.008 loss : 1.404975\n",
      "Epoch  360/1000 w1 0.687 w2 0.646 w3 0.678 b :0.008 loss : 1.399065\n",
      "Epoch  370/1000 w1 0.688 w2 0.645 w3 0.678 b :0.008 loss : 1.393182\n",
      "Epoch  380/1000 w1 0.688 w2 0.645 w3 0.678 b :0.008 loss : 1.387329\n",
      "Epoch  390/1000 w1 0.689 w2 0.644 w3 0.678 b :0.008 loss : 1.381518\n",
      "Epoch  400/1000 w1 0.689 w2 0.643 w3 0.678 b :0.008 loss : 1.375730\n",
      "Epoch  410/1000 w1 0.690 w2 0.643 w3 0.678 b :0.008 loss : 1.369968\n",
      "Epoch  420/1000 w1 0.690 w2 0.642 w3 0.678 b :0.008 loss : 1.364238\n",
      "Epoch  430/1000 w1 0.691 w2 0.642 w3 0.678 b :0.008 loss : 1.358543\n",
      "Epoch  440/1000 w1 0.691 w2 0.641 w3 0.678 b :0.008 loss : 1.352871\n",
      "Epoch  450/1000 w1 0.692 w2 0.641 w3 0.678 b :0.008 loss : 1.347241\n",
      "Epoch  460/1000 w1 0.692 w2 0.640 w3 0.678 b :0.008 loss : 1.341628\n",
      "Epoch  470/1000 w1 0.693 w2 0.640 w3 0.678 b :0.008 loss : 1.336044\n",
      "Epoch  480/1000 w1 0.693 w2 0.639 w3 0.678 b :0.008 loss : 1.330503\n",
      "Epoch  490/1000 w1 0.694 w2 0.638 w3 0.678 b :0.008 loss : 1.324995\n",
      "Epoch  500/1000 w1 0.694 w2 0.638 w3 0.678 b :0.009 loss : 1.319511\n",
      "Epoch  510/1000 w1 0.695 w2 0.637 w3 0.678 b :0.009 loss : 1.314054\n",
      "Epoch  520/1000 w1 0.695 w2 0.637 w3 0.678 b :0.009 loss : 1.308619\n",
      "Epoch  530/1000 w1 0.696 w2 0.636 w3 0.678 b :0.009 loss : 1.303219\n",
      "Epoch  540/1000 w1 0.696 w2 0.636 w3 0.678 b :0.009 loss : 1.297852\n",
      "Epoch  550/1000 w1 0.697 w2 0.635 w3 0.678 b :0.009 loss : 1.292500\n",
      "Epoch  560/1000 w1 0.697 w2 0.635 w3 0.679 b :0.009 loss : 1.287185\n",
      "Epoch  570/1000 w1 0.698 w2 0.634 w3 0.679 b :0.009 loss : 1.281912\n",
      "Epoch  580/1000 w1 0.698 w2 0.634 w3 0.679 b :0.009 loss : 1.276643\n",
      "Epoch  590/1000 w1 0.699 w2 0.633 w3 0.679 b :0.009 loss : 1.271422\n",
      "Epoch  600/1000 w1 0.699 w2 0.633 w3 0.679 b :0.009 loss : 1.266222\n",
      "Epoch  610/1000 w1 0.700 w2 0.632 w3 0.679 b :0.009 loss : 1.261043\n",
      "Epoch  620/1000 w1 0.700 w2 0.632 w3 0.679 b :0.009 loss : 1.255906\n",
      "Epoch  630/1000 w1 0.701 w2 0.631 w3 0.679 b :0.009 loss : 1.250787\n",
      "Epoch  640/1000 w1 0.701 w2 0.630 w3 0.679 b :0.009 loss : 1.245687\n",
      "Epoch  650/1000 w1 0.702 w2 0.630 w3 0.679 b :0.009 loss : 1.240615\n",
      "Epoch  660/1000 w1 0.702 w2 0.629 w3 0.679 b :0.009 loss : 1.235588\n",
      "Epoch  670/1000 w1 0.703 w2 0.629 w3 0.679 b :0.009 loss : 1.230566\n",
      "Epoch  680/1000 w1 0.703 w2 0.628 w3 0.679 b :0.009 loss : 1.225592\n",
      "Epoch  690/1000 w1 0.704 w2 0.628 w3 0.679 b :0.009 loss : 1.220646\n",
      "Epoch  700/1000 w1 0.704 w2 0.627 w3 0.679 b :0.009 loss : 1.215696\n",
      "Epoch  710/1000 w1 0.705 w2 0.627 w3 0.679 b :0.009 loss : 1.210797\n",
      "Epoch  720/1000 w1 0.705 w2 0.626 w3 0.679 b :0.009 loss : 1.205924\n",
      "Epoch  730/1000 w1 0.706 w2 0.626 w3 0.679 b :0.009 loss : 1.201071\n",
      "Epoch  740/1000 w1 0.706 w2 0.625 w3 0.679 b :0.009 loss : 1.196242\n",
      "Epoch  750/1000 w1 0.707 w2 0.625 w3 0.679 b :0.009 loss : 1.191435\n",
      "Epoch  760/1000 w1 0.707 w2 0.624 w3 0.679 b :0.009 loss : 1.186670\n",
      "Epoch  770/1000 w1 0.707 w2 0.624 w3 0.679 b :0.009 loss : 1.181923\n",
      "Epoch  780/1000 w1 0.708 w2 0.623 w3 0.679 b :0.009 loss : 1.177190\n",
      "Epoch  790/1000 w1 0.708 w2 0.623 w3 0.679 b :0.009 loss : 1.172482\n",
      "Epoch  800/1000 w1 0.709 w2 0.622 w3 0.679 b :0.009 loss : 1.167818\n",
      "Epoch  810/1000 w1 0.709 w2 0.622 w3 0.679 b :0.009 loss : 1.163170\n",
      "Epoch  820/1000 w1 0.710 w2 0.621 w3 0.680 b :0.009 loss : 1.158537\n",
      "Epoch  830/1000 w1 0.710 w2 0.621 w3 0.680 b :0.009 loss : 1.153941\n",
      "Epoch  840/1000 w1 0.711 w2 0.620 w3 0.680 b :0.009 loss : 1.149367\n",
      "Epoch  850/1000 w1 0.711 w2 0.620 w3 0.680 b :0.009 loss : 1.144810\n",
      "Epoch  860/1000 w1 0.712 w2 0.619 w3 0.680 b :0.009 loss : 1.140291\n",
      "Epoch  870/1000 w1 0.712 w2 0.619 w3 0.680 b :0.009 loss : 1.135794\n",
      "Epoch  880/1000 w1 0.713 w2 0.618 w3 0.680 b :0.009 loss : 1.131321\n",
      "Epoch  890/1000 w1 0.713 w2 0.618 w3 0.680 b :0.009 loss : 1.126860\n",
      "Epoch  900/1000 w1 0.713 w2 0.617 w3 0.680 b :0.009 loss : 1.122429\n",
      "Epoch  910/1000 w1 0.714 w2 0.617 w3 0.680 b :0.009 loss : 1.118013\n",
      "Epoch  920/1000 w1 0.714 w2 0.616 w3 0.680 b :0.009 loss : 1.113620\n",
      "Epoch  930/1000 w1 0.715 w2 0.616 w3 0.680 b :0.009 loss : 1.109264\n",
      "Epoch  940/1000 w1 0.715 w2 0.615 w3 0.680 b :0.009 loss : 1.104940\n",
      "Epoch  950/1000 w1 0.716 w2 0.615 w3 0.680 b :0.009 loss : 1.100610\n",
      "Epoch  960/1000 w1 0.716 w2 0.614 w3 0.680 b :0.009 loss : 1.096329\n",
      "Epoch  970/1000 w1 0.717 w2 0.614 w3 0.680 b :0.009 loss : 1.092049\n",
      "Epoch  980/1000 w1 0.717 w2 0.613 w3 0.680 b :0.009 loss : 1.087816\n",
      "Epoch  990/1000 w1 0.717 w2 0.613 w3 0.680 b :0.009 loss : 1.083577\n",
      "Epoch 1000/1000 w1 0.718 w2 0.613 w3 0.680 b :0.009 loss : 1.079378\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}